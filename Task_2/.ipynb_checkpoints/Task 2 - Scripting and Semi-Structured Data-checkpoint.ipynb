{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Descripition \n",
    "\n",
    "In 2012, URL shortening service Bitly partnered with the US government website USA.gov to provide a feed of anonymous data gathered from users who shorten links ending with .gov or .mil.\n",
    "\n",
    "The text file comes in JSON format and here are some keys and their description. They are only the most important ones for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|key| description |\n",
    "|---|-----------|\n",
    "| a|Denotes information about the web browser and operating system|\n",
    "| tz | time zone |\n",
    "| r | URL the user come from |\n",
    "| u | URL where the user headed to |\n",
    "| t | Timestamp when the user start using the website in UNIX format |\n",
    "| hc | Timestamp when user exit the website in UNIX format |\n",
    "| cy | City from which the request intiated |\n",
    "| ll | Longitude and Latitude |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All CSV files must have the following columns\n",
    "- web_browser\n",
    "        The web browser that has requested the service\n",
    "- operating_sys\n",
    "        operating system that intiated this request\n",
    "- from_url\n",
    "\n",
    "        The main URL the user came from\n",
    "\n",
    "    **note**:\n",
    "\n",
    "    If the retrived URL was in a long format `http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf`\n",
    "\n",
    "     make it appear in the file in a short format like this `www.facebook.com`\n",
    "     \n",
    "    \n",
    "- to_url\n",
    "\n",
    "       The same applied like `to_url`\n",
    "   \n",
    "- city\n",
    "\n",
    "        The city from which the the request was sent\n",
    "    \n",
    "- longitude\n",
    "\n",
    "        The longitude where the request was sent\n",
    "- latitude\n",
    "\n",
    "        The latitude where the request was sent\n",
    "\n",
    "- time_zone\n",
    "        \n",
    "        The time zone that the city follow\n",
    "        \n",
    "- time_in\n",
    "\n",
    "        Time when the request started\n",
    "- time_out\n",
    "        \n",
    "        Time when the request is ended\n",
    "        \n",
    "        \n",
    "**NOTE** :\n",
    "\n",
    "Because that some instances of the file are incomplete, you may encouter some NaN values in your transforamtion. Make sure that the final dataframes have no NaNs at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Script itself must do the following before and after trasforamtion: \n",
    "    \n",
    "- One positional argument which is the directory path with that have the files.\n",
    "\n",
    "\n",
    "- One optional argument **-u**. If this argument is passed will maintain the UNIX format of timpe stamp and if not                passed the time stamps will be converted.\n",
    "\n",
    "\n",
    "- Check if the files have any dublicates in between **checksum** and print a messeage that indicate that.\n",
    "\n",
    "\n",
    "- Print a message after converting each file with the number of rows transformed and the path of this file\n",
    "\n",
    "\n",
    "- At the end of this script print the total excution time.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell, I tried to provide some helper code for better understanding and clearer vision\n",
    "\n",
    "-**HINT**- Those lines of code may be not helping at all with your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script can transform the JSON files to a DataFrame and commit each file to a sparete CSV file in the target directory and consider the following:\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from pandas.io.json import json_normalize \n",
    "import json\n",
    "from subprocess import run, PIPE, Popen\n",
    "import subprocess\n",
    "import argparse\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to search for json files ==>  /mnt/d/ITI/python_for_data_management/Task_2\n",
      "files detected :\n",
      " ['usa.gov_click_data_1.json', 'usa.gov_click_data_2.json', 'usa.gov_click_data_3.json']\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(\"path to search for json files ==> \" , path)\n",
    "files=[]\n",
    "for file in listdir(path):\n",
    "    if '.json' in file:\n",
    "        files.append(file)\n",
    "print(\"files detected :\\n\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Duplicates: ['usa.gov_click_data_3.json']\n"
     ]
    }
   ],
   "source": [
    "checksums = {}\n",
    "duplicates = []\n",
    "\n",
    "for filename in files:\n",
    "    # Use Popen to call the md5sum utility\n",
    "    with Popen([\"md5sum\", filename], stdout=PIPE) as proc:\n",
    "        checksum, _ = proc.stdout.read().split()\n",
    "        \n",
    "        # Append duplicate to a list if the checksum is found\n",
    "        if checksum in checksums:\n",
    "            duplicates.append(filename)\n",
    "        checksums[checksum] = filename\n",
    "\n",
    "print(f\"Found Duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'c': 'US', 'nk': 1, 'tz': 'America/New_York', 'gr': 'MA', 'g': 'A6qOVH', 'h': 'wfLQtf', 'l': 'orofrog', 'al': 'en-US,en;q=0.8', 'hh': '1.usa.gov', 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991', 't': 1333307030, 'hc': 1333307037, 'cy': 'Danvers', 'll': [42.576698, -70.954903]}\n",
      "{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'c': 'US', 'nk': 1, 'tz': 'America/New_York', 'gr': 'MA', 'g': 'A6qOVH', 'h': 'wfLQtf', 'l': 'orofrog', 'al': 'en-US,en;q=0.8', 'hh': '1.usa.gov', 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991', 't': 1331923247, 'hc': 1331822918, 'cy': 'Danvers', 'll': [42.576698, -70.954903]}\n",
      "{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'c': 'US', 'nk': 1, 'tz': 'America/New_York', 'gr': 'MA', 'g': 'A6qOVH', 'h': 'wfLQtf', 'l': 'orofrog', 'al': 'en-US,en;q=0.8', 'hh': '1.usa.gov', 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991', 't': 1331923247, 'hc': 1331822918, 'cy': 'Danvers', 'll': [42.576698, -70.954903]}\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    records = [json.loads(line) for line in open(file)]\n",
    "    print(records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11',\n",
       " 'c': 'US',\n",
       " 'nk': 1,\n",
       " 'tz': 'America/New_York',\n",
       " 'gr': 'MA',\n",
       " 'g': 'A6qOVH',\n",
       " 'h': 'wfLQtf',\n",
       " 'l': 'orofrog',\n",
       " 'al': 'en-US,en;q=0.8',\n",
       " 'hh': '1.usa.gov',\n",
       " 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',\n",
       " 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991',\n",
       " 't': 1333307030,\n",
       " 'hc': 1333307037,\n",
       " 'cy': 'Danvers',\n",
       " 'll': [42.576698, -70.954903]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will try to retrieve one instance of the file in a list of dictionaries\n",
    "\n",
    "records = [json.loads(line) for line in open('usa.gov_click_data_1.json')]\n",
    "# Print the first occurance\n",
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "\n",
    "#parser.add_argument(\"dir\", help = \"Enter path of Directory\")\n",
    "\n",
    "#parser.add_argument(\"-u\", \"--unix\", action=\"store_true\", dest=\"unix\", default=False, help=\"This to manage time\")\n",
    "\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [json.loads(line) for line in open('usa.gov_click_data_1.json')]\n",
    "df = pd.DataFrame(records)\n",
    "df = df[['a','tz','r','u','t','ll','hc','cy']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['web_Browser']= df.a.str.extract(r'^([a-zA-Z]*/)')   #extract any ch end with /   \n",
    "df['web_Browser']= df.a.str.extract(r'^([a-zA-Z]*)')    #then remove   / extract chs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['operating_system']= df.a.str.extract(r'(\\([^(]+\\))', expand=True)\n",
    "lst = []\n",
    "Operating_System_lst = []\n",
    "for i in df['operating_system']:\n",
    "    i = str(i)\n",
    "    lst = i.split(\" \")\n",
    "    x = re.sub(\"\\(\",\"\",lst[0])\n",
    "    x = re.sub(\";\",\"\",x)\n",
    "    Operating_System_lst.append(x)\n",
    "df['operating_system']= Operating_System_lst\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "From_Url_lst=[]                                                       # OUTPut       //t.co/N\n",
    "df['from_url'] = df.r.str.extract(r'(//[a-zA-Z]*.[a-zA-Z]*.[a-zA-Z]*)', expand=True)   # output //www.facebook.com\n",
    "for i in df['from_url']:\n",
    "    i = str(i)\n",
    "    x = re.sub(\"//\",\"\",str(i))\n",
    "    if \"/\" in x:\n",
    "        x = x[0:x.index(\"/\")]\n",
    "        From_Url_lst.append(x)\n",
    "    else:\n",
    "        From_Url_lst.append(x)\n",
    "From_Url_lst\n",
    "df['from_url'] = From_Url_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['to_url'] = df.u.str.extract(r'(.*[.gov])', expand=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "To_Url_lst=[]\n",
    "df['to_url'] = df.u.str.extract(r'(.*[.gov])', expand=True)\n",
    "#df['to_url'] = df.u.str.extract(r'(//[a-zA-Z]*.[a-zA-Z]*.[a-zA-Z])', expand=True) # remove the beggining http // ..\n",
    "for i in df['to_url']:\n",
    "    i = str(i)\n",
    "    x = re.sub(\"http://\",\"\",str(i))\n",
    "    if \"/\" in x:\n",
    "        x = x[0:x.index(\"/\")]\n",
    "        To_Url_lst.append(x)\n",
    "    else:\n",
    "        To_Url_lst.append(x)\n",
    "From_Url_lst\n",
    "df['to_url'] = To_Url_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'tz' : 'time_zone', 'cy': 'city','t': 'time_in','hc':'time_out'}, inplace = True)\n",
    "df['ll'] = df['ll'].fillna('')\n",
    "df[['longitude','latitude']] = pd.DataFrame(df['ll'].values.tolist(), index = df.index)\n",
    "df = df[['web_Browser','operating_system','from_url','to_url','city','longitude','latitude','time_zone','time_in','time_out']]\n",
    "#df.replace(r'\\s+', np.nan)\n",
    "df = df.replace(to_replace = \"nan\" , value = np.nan)\n",
    "df = df.replace(to_replace = r'\\s+' , value = np.nan)\n",
    "df = df.dropna(axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
